# ğŸŒ Playwright çˆ¬è™«ä½¿ç”¨æŒ‡å—

## ğŸ“š æ¦‚è§ˆ

æœ¬æ¨¡å—æä¾›äº†ä¸¤ç§ç‰ˆæœ¬çš„ Playwright çˆ¬è™«ï¼š

1. **AsyncScrapyPlaywright** - å¼‚æ­¥ç‰ˆæœ¬ï¼ˆæ¨èç”¨äºé«˜å¹¶å‘åœºæ™¯ï¼‰
2. **SyncScrapyPlaywright** - åŒæ­¥ç‰ˆæœ¬ï¼ˆæ¨èç”¨äºç®€å•åœºæ™¯æˆ–è„šæœ¬ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¼‚æ­¥ç‰ˆæœ¬ï¼ˆAsyncScrapyPlaywrightï¼‰

é€‚ç”¨äºï¼š
- éœ€è¦é«˜æ€§èƒ½å’Œå¹¶å‘å¤„ç†
- å·²æœ‰å¼‚æ­¥ä»£ç ç¯å¢ƒ
- éœ€è¦åŒæ—¶çˆ¬å–å¤šä¸ªç½‘ç«™

```python
import asyncio
from AutoPPT.scrapy.playwright import AsyncScrapyPlaywright

async def main():
    scrapy = AsyncScrapyPlaywright()
    
    await scrapy.start(
        target_url="https://example.com",
        extracted_content_file="content.txt",
        images_downloaded_dir="images"
    )
    
    print("çˆ¬å–å®Œæˆï¼")

# è¿è¡Œå¼‚æ­¥å‡½æ•°
if __name__ == "__main__":
    asyncio.run(main())
```

### 2. åŒæ­¥ç‰ˆæœ¬ï¼ˆSyncScrapyPlaywrightï¼‰

é€‚ç”¨äºï¼š
- ç®€å•çš„çˆ¬è™«è„šæœ¬
- é¡ºåºæ‰§è¡Œçš„åœºæ™¯
- ä¸éœ€è¦å¤„ç†å¼‚æ­¥çš„æƒ…å†µ

```python
from AutoPPT.scrapy.playwright import SyncScrapyPlaywright

def main():
    scrapy = SyncScrapyPlaywright()
    
    scrapy.start(
        target_url="https://example.com",
        extracted_content_file="content.txt",
        images_downloaded_dir="images"
    )
    
    print("çˆ¬å–å®Œæˆï¼")

if __name__ == "__main__":
    main()
```

## ğŸ“– è¯¦ç»†ä½¿ç”¨

### åŸºæœ¬å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | è¯´æ˜ | å¿…å¡« |
|------|------|------|------|
| `target_url` | str | ç›®æ ‡ç½‘é¡µURL | âœ… |
| `extracted_content_file` | str | æå–å†…å®¹ä¿å­˜è·¯å¾„ | âœ… |
| `images_downloaded_dir` | str | å›¾ç‰‡ä¸‹è½½ç›®å½• | âœ… |

### è¾“å‡ºæ–‡ä»¶

#### 1. æå–çš„æ–‡å­—å†…å®¹
```
extracted_content.txt
â”œâ”€â”€ === æ–‡å­—å†…å®¹ ===
â”œâ”€â”€ æ–‡å­—1
â”œâ”€â”€ æ–‡å­—2
â””â”€â”€ ...
```

#### 2. ä¸‹è½½çš„å›¾ç‰‡
```
images_downloaded_dir/
â”œâ”€â”€ abc123.jpg           # å¤„ç†åçš„å›¾ç‰‡
â”œâ”€â”€ def456.webp
â””â”€â”€ ...

images_downloaded_dir_original_images/
â”œâ”€â”€ original_abc123.jpg  # åŸå§‹å›¾ç‰‡
â”œâ”€â”€ original_def456.webp
â””â”€â”€ ...
```

#### 3. å…¨é¡µé¢æˆªå›¾ï¼ˆå½“å›¾ç‰‡ä¸è¶³æ—¶ï¼‰
```
images_downloaded_dir/
â”œâ”€â”€ screenshot_000.jpg
â”œâ”€â”€ screenshot_001.jpg
â””â”€â”€ ...
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. å¹¶å‘çˆ¬å–å¤šä¸ªç½‘ç«™ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰

```python
import asyncio
from AutoPPT.scrapy.playwright import AsyncScrapyPlaywright

async def scrape_single(url: str, output_dir: str):
    """çˆ¬å–å•ä¸ªç½‘ç«™"""
    scrapy = AsyncScrapyPlaywright()
    
    content_file = f"{output_dir}/content.txt"
    images_dir = f"{output_dir}/images"
    
    await scrapy.start(url, content_file, images_dir)

async def scrape_multiple(urls: list):
    """å¹¶å‘çˆ¬å–å¤šä¸ªç½‘ç«™"""
    tasks = []
    
    for i, url in enumerate(urls):
        output_dir = f"output_{i}"
        task = scrape_single(url, output_dir)
        tasks.append(task)
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    await asyncio.gather(*tasks)
    
    print(f"å®Œæˆ {len(urls)} ä¸ªç½‘ç«™çš„çˆ¬å–ï¼")

# ä½¿ç”¨ç¤ºä¾‹
urls = [
    "https://example1.com",
    "https://example2.com",
    "https://example3.com",
]

asyncio.run(scrape_multiple(urls))
```

### 2. æ‰¹é‡çˆ¬å–ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

```python
from AutoPPT.scrapy.playwright import SyncScrapyPlaywright

def scrape_batch(urls: list):
    """æ‰¹é‡çˆ¬å–ç½‘ç«™"""
    scrapy = SyncScrapyPlaywright()
    
    for i, url in enumerate(urls):
        print(f"\n[{i+1}/{len(urls)}] çˆ¬å–: {url}")
        
        try:
            scrapy.start(
                target_url=url,
                extracted_content_file=f"output_{i}/content.txt",
                images_downloaded_dir=f"output_{i}/images"
            )
            print(f"âœ… å®Œæˆ: {url}")
            
        except Exception as e:
            print(f"âŒ å¤±è´¥: {url} - {e}")
            continue
    
    print(f"\næ‰¹é‡çˆ¬å–å®Œæˆï¼")

# ä½¿ç”¨ç¤ºä¾‹
urls = [
    "https://example1.com",
    "https://example2.com",
    "https://example3.com",
]

scrape_batch(urls)
```

### 3. è‡ªå®šä¹‰é…ç½®

```python
from AutoPPT.scrapy.playwright import SyncScrapyPlaywright

class CustomScrapy(SyncScrapyPlaywright):
    """è‡ªå®šä¹‰çˆ¬è™«é…ç½®"""
    
    def start(self, target_url, extracted_content_file, images_downloaded_dir):
        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {target_url}")
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        super().start(target_url, extracted_content_file, images_downloaded_dir)
        
        print(f"âœ… çˆ¬å–å®Œæˆ: {target_url}")

# ä½¿ç”¨è‡ªå®šä¹‰çˆ¬è™«
scrapy = CustomScrapy()
scrapy.start("https://example.com", "content.txt", "images")
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯å¯¹æ¯”

### ä½•æ—¶ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ï¼ˆAsyncScrapyPlaywrightï¼‰

âœ… **æ¨èåœºæ™¯**ï¼š
- éœ€è¦åŒæ—¶çˆ¬å–å¤šä¸ªç½‘ç«™
- é«˜å¹¶å‘éœ€æ±‚
- ä¸å…¶ä»–å¼‚æ­¥ä»£ç é›†æˆ
- éœ€è¦æœ€å¤§åŒ–æ€§èƒ½

âŒ **ä¸æ¨è**ï¼š
- ç®€å•çš„å•æ¬¡çˆ¬å–
- ä¸ç†Ÿæ‚‰å¼‚æ­¥ç¼–ç¨‹
- é¡ºåºå¤„ç†å°±è¶³å¤Ÿ

**ç¤ºä¾‹**ï¼š
```python
# å¹¶å‘çˆ¬å– 10 ä¸ªç½‘ç«™ï¼Œé€Ÿåº¦å¿«
urls = [f"https://example{i}.com" for i in range(10)]
await asyncio.gather(*[scrape(url) for url in urls])
```

### ä½•æ—¶ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬ï¼ˆSyncScrapyPlaywrightï¼‰

âœ… **æ¨èåœºæ™¯**ï¼š
- ç®€å•çš„è„šæœ¬ä»»åŠ¡
- å•ä¸ªæˆ–å°‘é‡ç½‘ç«™çˆ¬å–
- é¡ºåºå¤„ç†å³å¯
- ä»£ç ç®€å•æ˜“æ‡‚

âŒ **ä¸æ¨è**ï¼š
- éœ€è¦é«˜å¹¶å‘
- æ€§èƒ½è¦æ±‚é«˜
- å·²æœ‰å¼‚æ­¥æ¶æ„

**ç¤ºä¾‹**ï¼š
```python
# ç®€å•ç›´æ¥ï¼Œä¸€ä¸ªæ¥ä¸€ä¸ªçˆ¬å–
for url in urls:
    scrapy.start(url, ...)
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | å¼‚æ­¥ç‰ˆæœ¬ | åŒæ­¥ç‰ˆæœ¬ | æ€§èƒ½å·®å¼‚ |
|------|----------|----------|----------|
| å•ä¸ªç½‘ç«™ | ~10ç§’ | ~10ç§’ | æ— å·®å¼‚ |
| 3ä¸ªç½‘ç«™ï¼ˆå¹¶å‘ï¼‰ | ~12ç§’ | ~30ç§’ | **2.5x** â¬†ï¸ |
| 10ä¸ªç½‘ç«™ï¼ˆå¹¶å‘ï¼‰ | ~15ç§’ | ~100ç§’ | **6.7x** â¬†ï¸ |
| å†…å­˜å ç”¨ | ç¨é«˜ | æ­£å¸¸ | - |
| ä»£ç å¤æ‚åº¦ | ä¸­ | ä½ | - |

## ğŸ” åŠŸèƒ½ç‰¹æ€§

### ä¸¤ä¸ªç‰ˆæœ¬å…±åŒç‰¹æ€§

âœ… **è‡ªåŠ¨ä»£ç†æ£€æµ‹å’Œåˆ‡æ¢**
- è‡ªåŠ¨æµ‹è¯•ç›®æ ‡URLæ˜¯å¦å¯è®¿é—®
- å¤±è´¥æ—¶è‡ªåŠ¨å°è¯•ä»£ç†åˆ—è¡¨
- æ™ºèƒ½é€‰æ‹©å¯ç”¨ä»£ç†

âœ… **æ™ºèƒ½å›¾ç‰‡ä¸‹è½½**
- æ‹¦æˆªæ‰€æœ‰å›¾ç‰‡å“åº”
- æ”¯æŒ JPGã€JPEGã€WebP æ ¼å¼
- è‡ªåŠ¨è¿‡æ»¤å¤ªå°çš„å›¾ç‰‡ï¼ˆ<350x350ï¼‰
- è‡ªåŠ¨è¿‡æ»¤å¤ªå¤§çš„æ–‡ä»¶ï¼ˆ>13MBï¼‰
- ä¿å­˜åŸå›¾å’Œå¤„ç†åçš„å›¾ç‰‡

âœ… **å…¨é¡µé¢æˆªå›¾**
- å›¾ç‰‡ä¸è¶³æ—¶è‡ªåŠ¨å¯ç”¨
- åˆ†æ®µæˆªå›¾ï¼Œé¿å…å•å¼ è¿‡å¤§
- æœ€å¤š30å¼ æˆªå›¾

âœ… **å†…å®¹æå–**
- æå–æ‰€æœ‰å¯è§æ–‡å­—
- è¿‡æ»¤å¯¼èˆªã€é¡µè„šç­‰æ— å…³å†…å®¹
- ä¿å­˜ä¸ºç»“æ„åŒ–æ–‡æœ¬

âœ… **åçˆ¬è™«ç‰¹æ€§**
- éšæœº User-Agent
- å®Œæ•´çš„æµè§ˆå™¨ç‰¹å¾æ¨¡æ‹Ÿ
- Playwright Stealth æ’ä»¶
- è‡ªç„¶çš„æ»šåŠ¨è¡Œä¸º

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. ä¾èµ–å®‰è£…

```bash
# å®‰è£… Playwright
pip install playwright playwright-stealth

# å®‰è£…æµè§ˆå™¨
playwright install chromium
```

### 2. å¼‚æ­¥ç‰ˆæœ¬æ³¨æ„äº‹é¡¹

```python
# âŒ é”™è¯¯ï¼šå¿˜è®° await
scrapy = AsyncScrapyPlaywright()
scrapy.start(...)  # è¿”å› coroutineï¼Œä¸ä¼šæ‰§è¡Œ

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ await
await scrapy.start(...)

# âŒ é”™è¯¯ï¼šåœ¨éå¼‚æ­¥å‡½æ•°ä¸­ä½¿ç”¨
def main():
    await scrapy.start(...)  # SyntaxError

# âœ… æ­£ç¡®ï¼šåœ¨å¼‚æ­¥å‡½æ•°ä¸­ä½¿ç”¨
async def main():
    await scrapy.start(...)
```

### 3. åŒæ­¥ç‰ˆæœ¬æ³¨æ„äº‹é¡¹

```python
# âŒ é”™è¯¯ï¼šå°è¯•å¹¶å‘
for url in urls:
    # è¿™æ˜¯é¡ºåºæ‰§è¡Œï¼Œä¸æ˜¯å¹¶å‘
    scrapy.start(url, ...)

# âœ… å¦‚éœ€å¹¶å‘ï¼Œä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬
```

### 4. èµ„æºç®¡ç†

```python
# âœ… æ¨èï¼šä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ç›®å½•
for i, url in enumerate(urls):
    scrapy.start(
        url,
        f"output_{i}/content.txt",
        f"output_{i}/images"
    )
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©å¼‚æ­¥è¿˜æ˜¯åŒæ­¥ï¼Ÿ

**A**: 
- çˆ¬å–1-3ä¸ªç½‘ç«™ â†’ åŒæ­¥ç‰ˆæœ¬
- çˆ¬å–4ä¸ªä»¥ä¸Šç½‘ç«™ â†’ å¼‚æ­¥ç‰ˆæœ¬
- éœ€è¦é›†æˆåˆ°å¼‚æ­¥é¡¹ç›® â†’ å¼‚æ­¥ç‰ˆæœ¬
- ç®€å•è„šæœ¬ â†’ åŒæ­¥ç‰ˆæœ¬

### Q2: å¼‚æ­¥ç‰ˆæœ¬æŠ¥é”™ "RuntimeError: asyncio.run() cannot be called from a running event loop"

**A**: 
```python
# åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨
import nest_asyncio
nest_asyncio.apply()

await scrapy.start(...)  # ç›´æ¥ awaitï¼Œä¸ç”¨ asyncio.run()
```

### Q3: å¦‚ä½•ç¦ç”¨ä»£ç†ï¼Ÿ

**A**: 
```python
# ä¿®æ”¹ SimpleProxyManager
proxy_manager = SimpleProxyManager()
proxy_manager.proxies = []  # æ¸…ç©ºä»£ç†åˆ—è¡¨
```

### Q4: å¦‚ä½•å¢åŠ è¶…æ—¶æ—¶é—´ï¼Ÿ

**A**: 
```python
# åœ¨ä»£ç ä¸­ä¿®æ”¹
page.set_default_timeout(120000)  # æ”¹ä¸º120ç§’
```

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç®€å•çˆ¬å–ï¼ˆåŒæ­¥ï¼‰

```python
from AutoPPT.scrapy.playwright import SyncScrapyPlaywright

scrapy = SyncScrapyPlaywright()
scrapy.start(
    target_url="https://example.com",
    extracted_content_file="content.txt",
    images_downloaded_dir="images"
)
```

### ç¤ºä¾‹2ï¼šå¹¶å‘çˆ¬å–ï¼ˆå¼‚æ­¥ï¼‰

```python
import asyncio
from AutoPPT.scrapy.playwright import AsyncScrapyPlaywright

async def main():
    urls = ["https://example1.com", "https://example2.com"]
    scrapy = AsyncScrapyPlaywright()
    
    tasks = []
    for i, url in enumerate(urls):
        task = scrapy.start(
            url,
            f"output_{i}/content.txt",
            f"output_{i}/images"
        )
        tasks.append(task)
    
    await asyncio.gather(*tasks)

asyncio.run(main())
```

### ç¤ºä¾‹3ï¼šé”™è¯¯å¤„ç†

```python
from AutoPPT.scrapy.playwright import SyncScrapyPlaywright

scrapy = SyncScrapyPlaywright()

try:
    scrapy.start(
        target_url="https://example.com",
        extracted_content_file="content.txt",
        images_downloaded_dir="images"
    )
    print("âœ… çˆ¬å–æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
    # å¤„ç†é”™è¯¯...
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [Playwright å®˜æ–¹æ–‡æ¡£](https://playwright.dev/python/)
- [Playwright Stealth](https://github.com/AtuboDad/playwright_stealth)
- [Python å¼‚æ­¥ç¼–ç¨‹æŒ‡å—](https://docs.python.org/3/library/asyncio.html)

---

**æœ€åæ›´æ–°**: 2025-10-17  
**ç»´æŠ¤è€…**: æ™ºé€ æ¥­ john

